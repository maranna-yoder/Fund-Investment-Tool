LoadFunction <- function(path){
load(path)
return(combined_data)
}
closet_data <- lapply(paths, LoadFunction) %>% bind_rows
table(closet_data$user)
# Load closets
paths <- c("reseller_a_closet_20200721.RDa",
"reseller_c_closet_20200723.RDa",
"reseller_b_closet_20200724.RDa")
LoadFunction <- function(path){
load(path)
return(combined_data)
}
closet_data <- lapply(paths, LoadFunction) %>% bind_rows
closet_data %<>% mutate(user = ifelse(user == "mogibeth", "A",
ifelse(user == "emptyhanger", "B",
ifelse(user == "nicolestate", "C", NA))))
table(closet_data$user)
date_start <- "2019-07-01"
date_end <- "2020-06-30"
active_summary <- closet_data %>%
filter(status == "Available") %>%
group_by(user) %>%
summarize(active_listings = n())
sold_summary <- closet_data %>%
filter(status == "Sold") %>%
filter(date_sold >= date_start & date_sold <= date_end) %>%
group_by(user) %>%
summarize(
total_sales   = sum(price)/1000,
total_revenue = sum(revenue)/1000,
items_sold    = n()) %>%
mutate(asp      = total_sales*1000 / items_sold)
active_summary <- closet_data %>%
filter(status == "Available") %>%
group_by(user) %>%
summarize(active_listings = n())
sold_summary <- closet_data %>%
filter(status == "Sold") %>%
filter(date_sold >= date_start & date_sold <= date_end) %>%
group_by(user) %>%
summarize(
total_sales   = sum(price)/1000,
items_sold    = n()) %>%
mutate(asp      = total_sales*1000 / items_sold)
View(active_summary)
View(sold_summary)
overall_summary <- left_join(active_summary, sold_summary)
overall_summary <- left_join(active_summary, sold_summary, by = "user")
date_start <- "2019-07-01"
date_end <- "2020-06-30"
active_summary <- closet_data %>%
filter(status == "Available") %>%
group_by(user) %>%
summarize(active_listings = n())
sold_summary <- closet_data %>%
filter(status == "Sold") %>%
filter(date_sold >= date_start & date_sold <= date_end) %>%
group_by(user) %>%
summarize(
total_sales   = sum(price)/1000,
items_sold    = n()) %>%
mutate(asp      = total_sales*1000 / items_sold)
overall_summary <- left_join(active_summary, sold_summary, by = "user")
kable(overall_summary)
?kable
test <- readRDS("C:/Users/maran/Dropbox/Web Scraping/inter/just_sold/results_2020-08-06.RDS")
test <- readRDS("C:/Users/maran/Dropbox/Web Scraping/inter/just_sold/results_2020-08-06_2000.RDS")
View(test)
test <- readRDS("C:/Users/maran/Dropbox/Web Scraping/inter/just_sold/results_subcategory_2020-08-06_1330.RDS")
test <- readRDS("C:/Users/maran/Dropbox/Web Scraping/inter/just_sold/results_subcategory_2020-08-06_1440.RDS")
View(test)
table(test$super_category)
sum(is.na(test))
sum(is.na(test$super_category))
sum(is.na(test$title))
sum(is.na(test$price))
sum(is.na(test$market))
sum(is.na(test$item_url))
sum(is.na(test$date_posted))
sum(is.na(test$size))
sum(is.na(test$nwt))
sum(is.na(test$status))
sum(is.na(test$scrape_time))
sum(is.na(test$item_id))
sum(is.na(test$item_url))
sum(is.na(test$search_url))
sum(is.na(test$scrape_id))
sum(is.na(test$days_to_sell))
sum(is.na(test$date_sold))
sum(is.na(test$scrape_time))
sum(is.na(test$user))
sum(is.na(test$price_range))
sum(is.na(test$category))
sum(is.na(test$subcategory))
sum(is.na(test$boutique))
sum(is.na(test$nwt))
str(test)
glimpse(test)
library(dplyr)
glimpse(test)
sum(is.na(test))
sum(is.na(test$brand))
install.packages(c("backports", "broom", "callr", "cpp11", "data.table", "dbplyr", "ellipsis", "fs", "ggplot2", "glue", "gt", "haven", "htmltools", "httpuv", "httr", "isoband", "jsonlite", "later", "lubridate", "modelr", "openssl", "pillar", "pkgbuild", "pkgload", "processx", "promises", "ps", "purrr", "Rcpp", "reshape2", "rJava", "robotstxt", "rvest", "scales", "shiny", "SnowballC", "stopwords", "sys", "textdata", "tidytext", "tinytex", "withr", "writexl", "xml2", "zoo"))
library(updateR)
install.packages("updateR")
library("installr")
library(dplyr)
install.packages("dplyr")
install.packages("installr")
install.packages("lubridate")
install.packages("tidyverse")
install.packages("tidyquant")
install.packages("openxlsx")
# Rescrape price and size information from missing spots
library(dplyr)
library(magrittr)
library(rvest)
setwd("C:/Users/maran/Documents/Data Stuff/Web Scraping/Scraped datasets")
rm(list = ls())
gc()
date_sold_start <- "2020-07-05"
date_sold_end   <- "2020-07-05"
# Load data
scraped_data <- readRDS("scraped_2020-07_ALL.RDS")
missing_price_all <- scraped_data %>%
filter(is.na(price)) %>%
select(item_id, title, item_url, date_sold, user)
missing_price <- missing_price_all %>%
filter(date_sold >= date_sold_start & date_sold <= date_sold_end)
missing_price %<>% mutate(item_url = paste0("http://www.poshmark.com/", item_url))
# Nodes to Text Wrapper
NodesToText <- function(page, tag){
html_data <- html_nodes(page, tag)
text <- html_text(html_data) %>%
as.character() %>% trimws
if(length(text) == 0) text <- NA
return(text)
}
# Function to scrape bits on an individual page
ScrapePriceSize <- function(item_url, item_id){
i <<- i+1
if(floor(i/10) == i/10) cat(paste("Scraping item", i,"\n"))
now <- Sys.time()
webpage <- try(read_html(item_url))
if(unique(class(webpage) == "try-error")){
cat(paste("Item", i, "URL was not accessible, returning NULL \n"))
results <- data.frame(item_id = item_id,
title = NA,
price = NA,
size = NA,
item_url = item_url,
date_scraped = now,
stringsAsFactors = F)
q <- runif(1, min = 0.1, max = 1)
Sys.sleep(1+q)
rm(webpage)
gc()
return(results)
}
# Grabbing price and size info
price <- NodesToText(webpage, ".listing__ipad-centered h1")
price <- strsplit(price, "\n")[[1]][1] %>% gsub("$", "", ., fixed = T) %>%
gsub(",", "", ., fixed = T) %>% as.numeric
size <- NodesToText(webpage, ".listing__size-selector-con")
title <- NodesToText(webpage, ".listing__title .fw--light")
results <- data.frame(item_id = item_id,
title = title,
price = price,
size = size,
item_url = item_url,
date_scraped = now,
stringsAsFactors = F)
if(nrow(results) !=1) cat(paste("Warning: item", title, "does not have 1 entry \n"))
q <- runif(1, min = 0.1, max = 1)
#cat(paste("sleeping", 3+q, "seconds \n"))
Sys.sleep(1+q)
return(results)
}
# Actually scrape it
# Number of obs to start with
num <- nrow(missing_price)
start_time <- Sys.time()
i <- 0
results <- mapply(ScrapePriceSize, missing_price$item_url[1:num], missing_price$item_id[1:num], SIMPLIFY = FALSE) %>% bind_rows
end_time <- Sys.time()
end_time - start_time
leftover <- results %>% filter(is.na(price))
results2 <- mapply(ScrapePriceSize, leftover$item_url, leftover$item_id, SIMPLIFY = FALSE) %>% bind_rows
View(results2)
full_results <- bind_rows(results, results2) %>% filter(!is.na(price)) %>% select(-title)
# Join back with the info before
price_results <- left_join(missing_price, full_results, by = c("item_id", "item_url")) %>% filter(!is.na(price))
save(price_results, file ="./price rescrape files/prices_2020-07-05.RDa")
# Rescrape price and size information from missing spots
library(dplyr)
library(magrittr)
library(rvest)
setwd("C:/Users/maran/Documents/Data Stuff/Web Scraping/Scraped datasets")
rm(list = ls())
gc()
date_sold_start <- "2020-07-04"
date_sold_end   <- "2020-07-04"
# Load data
scraped_data <- readRDS("scraped_2020-07_ALL.RDS")
missing_price_all <- scraped_data %>%
filter(is.na(price)) %>%
select(item_id, title, item_url, date_sold, user)
missing_price <- missing_price_all %>%
filter(date_sold >= date_sold_start & date_sold <= date_sold_end)
missing_price %<>% mutate(item_url = paste0("http://www.poshmark.com/", item_url))
# Nodes to Text Wrapper
NodesToText <- function(page, tag){
html_data <- html_nodes(page, tag)
text <- html_text(html_data) %>%
as.character() %>% trimws
if(length(text) == 0) text <- NA
return(text)
}
# Function to scrape bits on an individual page
ScrapePriceSize <- function(item_url, item_id){
i <<- i+1
if(floor(i/10) == i/10) cat(paste("Scraping item", i,"\n"))
now <- Sys.time()
webpage <- try(read_html(item_url))
if(unique(class(webpage) == "try-error")){
cat(paste("Item", i, "URL was not accessible, returning NULL \n"))
results <- data.frame(item_id = item_id,
title = NA,
price = NA,
size = NA,
item_url = item_url,
date_scraped = now,
stringsAsFactors = F)
q <- runif(1, min = 0.1, max = 1)
Sys.sleep(1+q)
rm(webpage)
gc()
return(results)
}
# Grabbing price and size info
price <- NodesToText(webpage, ".listing__ipad-centered h1")
price <- strsplit(price, "\n")[[1]][1] %>% gsub("$", "", ., fixed = T) %>%
gsub(",", "", ., fixed = T) %>% as.numeric
size <- NodesToText(webpage, ".listing__size-selector-con")
title <- NodesToText(webpage, ".listing__title .fw--light")
results <- data.frame(item_id = item_id,
title = title,
price = price,
size = size,
item_url = item_url,
date_scraped = now,
stringsAsFactors = F)
if(nrow(results) !=1) cat(paste("Warning: item", title, "does not have 1 entry \n"))
q <- runif(1, min = 0.1, max = 1)
#cat(paste("sleeping", 3+q, "seconds \n"))
Sys.sleep(1+q)
return(results)
}
# Actually scrape it
# Number of obs to start with
num <- nrow(missing_price)
start_time <- Sys.time()
i <- 0
results <- mapply(ScrapePriceSize, missing_price$item_url[1:num], missing_price$item_id[1:num], SIMPLIFY = FALSE) %>% bind_rows
end_time <- Sys.time()
end_time - start_time
# Redoing the ones that didn't work the first time
leftover <- results %>% filter(is.na(price))
results2 <- mapply(ScrapePriceSize, leftover$item_url, leftover$item_id, SIMPLIFY = FALSE) %>% bind_rows
View(results2)
full_results <- bind_rows(results, results2) %>% filter(!is.na(price)) %>% select(-title)
# Join back with the info before
price_results <- left_join(missing_price, full_results, by = c("item_id", "item_url")) %>% filter(!is.na(price))
save(price_results, file ="./price rescrape files/prices_2020-07-04.RDa")
# Rescrape price and size information from missing spots
library(dplyr)
library(magrittr)
library(rvest)
setwd("C:/Users/maran/Documents/Data Stuff/Web Scraping/Scraped datasets")
rm(list = ls())
gc()
date_sold_start <- "2020-07-03"
date_sold_end   <- "2020-07-03"
# Load data
scraped_data <- readRDS("scraped_2020-07_ALL.RDS")
missing_price_all <- scraped_data %>%
filter(is.na(price)) %>%
select(item_id, title, item_url, date_sold, user)
missing_price <- missing_price_all %>%
filter(date_sold >= date_sold_start & date_sold <= date_sold_end)
missing_price %<>% mutate(item_url = paste0("http://www.poshmark.com/", item_url))
# Nodes to Text Wrapper
NodesToText <- function(page, tag){
html_data <- html_nodes(page, tag)
text <- html_text(html_data) %>%
as.character() %>% trimws
if(length(text) == 0) text <- NA
return(text)
}
# Function to scrape bits on an individual page
ScrapePriceSize <- function(item_url, item_id){
i <<- i+1
if(floor(i/10) == i/10) cat(paste("Scraping item", i,"\n"))
now <- Sys.time()
webpage <- try(read_html(item_url))
if(unique(class(webpage) == "try-error")){
cat(paste("Item", i, "URL was not accessible, returning NULL \n"))
results <- data.frame(item_id = item_id,
title = NA,
price = NA,
size = NA,
item_url = item_url,
date_scraped = now,
stringsAsFactors = F)
q <- runif(1, min = 0.1, max = 1)
Sys.sleep(1+q)
rm(webpage)
gc()
return(results)
}
# Grabbing price and size info
price <- NodesToText(webpage, ".listing__ipad-centered h1")
price <- strsplit(price, "\n")[[1]][1] %>% gsub("$", "", ., fixed = T) %>%
gsub(",", "", ., fixed = T) %>% as.numeric
size <- NodesToText(webpage, ".listing__size-selector-con")
title <- NodesToText(webpage, ".listing__title .fw--light")
results <- data.frame(item_id = item_id,
title = title,
price = price,
size = size,
item_url = item_url,
date_scraped = now,
stringsAsFactors = F)
if(nrow(results) !=1) cat(paste("Warning: item", title, "does not have 1 entry \n"))
q <- runif(1, min = 0.1, max = 1)
#cat(paste("sleeping", 3+q, "seconds \n"))
Sys.sleep(1+q)
return(results)
}
# Actually scrape it
# Number of obs to start with
num <- nrow(missing_price)
start_time <- Sys.time()
i <- 0
results <- mapply(ScrapePriceSize, missing_price$item_url[1:num], missing_price$item_id[1:num], SIMPLIFY = FALSE) %>% bind_rows
end_time <- Sys.time()
end_time - start_time
# Redoing the ones that didn't work the first time
leftover <- results %>% filter(is.na(price))
results2 <- mapply(ScrapePriceSize, leftover$item_url, leftover$item_id, SIMPLIFY = FALSE) %>% bind_rows
View(results2)
full_results <- bind_rows(results, results2) %>% filter(!is.na(price)) %>% select(-title)
# Join back with the info before
price_results <- left_join(missing_price, full_results, by = c("item_id", "item_url")) %>% filter(!is.na(price))
save(price_results, file ="./price rescrape files/prices_2020-07-03.RDa")
# Rescrape price and size information from missing spots
library(dplyr)
library(magrittr)
library(rvest)
setwd("C:/Users/maran/Documents/Data Stuff/Web Scraping/Scraped datasets")
rm(list = ls())
gc()
date_sold_start <- "2020-07-02"
date_sold_end   <- "2020-07-02"
# Load data
scraped_data <- readRDS("scraped_2020-07_ALL.RDS")
missing_price_all <- scraped_data %>%
filter(is.na(price)) %>%
select(item_id, title, item_url, date_sold, user)
missing_price <- missing_price_all %>%
filter(date_sold >= date_sold_start & date_sold <= date_sold_end)
missing_price %<>% mutate(item_url = paste0("http://www.poshmark.com/", item_url))
# Nodes to Text Wrapper
NodesToText <- function(page, tag){
html_data <- html_nodes(page, tag)
text <- html_text(html_data) %>%
as.character() %>% trimws
if(length(text) == 0) text <- NA
return(text)
}
# Function to scrape bits on an individual page
ScrapePriceSize <- function(item_url, item_id){
i <<- i+1
if(floor(i/10) == i/10) cat(paste("Scraping item", i,"\n"))
now <- Sys.time()
webpage <- try(read_html(item_url))
if(unique(class(webpage) == "try-error")){
cat(paste("Item", i, "URL was not accessible, returning NULL \n"))
results <- data.frame(item_id = item_id,
title = NA,
price = NA,
size = NA,
item_url = item_url,
date_scraped = now,
stringsAsFactors = F)
q <- runif(1, min = 0.1, max = 1)
Sys.sleep(1+q)
rm(webpage)
gc()
return(results)
}
# Grabbing price and size info
price <- NodesToText(webpage, ".listing__ipad-centered h1")
price <- strsplit(price, "\n")[[1]][1] %>% gsub("$", "", ., fixed = T) %>%
gsub(",", "", ., fixed = T) %>% as.numeric
size <- NodesToText(webpage, ".listing__size-selector-con")
title <- NodesToText(webpage, ".listing__title .fw--light")
results <- data.frame(item_id = item_id,
title = title,
price = price,
size = size,
item_url = item_url,
date_scraped = now,
stringsAsFactors = F)
if(nrow(results) !=1) cat(paste("Warning: item", title, "does not have 1 entry \n"))
q <- runif(1, min = 0.1, max = 1)
#cat(paste("sleeping", 3+q, "seconds \n"))
Sys.sleep(1+q)
return(results)
}
# Actually scrape it
# Number of obs to start with
num <- nrow(missing_price)
start_time <- Sys.time()
i <- 0
results <- mapply(ScrapePriceSize, missing_price$item_url[1:num], missing_price$item_id[1:num], SIMPLIFY = FALSE) %>% bind_rows
end_time <- Sys.time()
end_time - start_time
# Redoing the ones that didn't work the first time
leftover <- results %>% filter(is.na(price))
results2 <- mapply(ScrapePriceSize, leftover$item_url, leftover$item_id, SIMPLIFY = FALSE) %>% bind_rows
View(results2)
full_results <- bind_rows(results, results2) %>% filter(!is.na(price)) %>% select(-title)
# Join back with the info before
price_results <- left_join(missing_price, full_results, by = c("item_id", "item_url")) %>% filter(!is.na(price))
save(price_results, file ="./price rescrape files/prices_2020-07-02.RDa")
results <- readRDS("C:/Users/maran/Dropbox/Web Scraping/inter/just_sold/results_2020-08-04_1200.RDS")
View(results)
funds <- read.csv("funds.csv", stringsAsFactors = F)
library(tidyquant)
library(dplyr)
library(xlsx)
library(tidyr)
rm(list = ls())
gc()
setwd("C:/Users/maran/Documents/Data Stuff/Fund Investment Tool")
wb <- loadWorkbook(file = "Investment Analysis Tool.xlsx")
funds <- read.csv("funds.csv", stringsAsFactors = F)
tickers <- funds %>% pull(ticker)
six_months <- today() - months(6)
year_start <- as.Date("2020-01-01")
earliest_date <- min(six_months, year_start)
today <- today()
stocks <- tq_get(tickers,
from = earliest_date,
to = today,
get = "stock.prices")
# Calculate return year to date
ytd_return <- stocks %>%
group_by(symbol) %>%
filter(date >= year_start) %>%
filter(!is.na(close)) %>%
tq_transmute(select = close,
mutate_fun = periodReturn,
period = "yearly",
type = "arithmetic") %>%
mutate(type = "ytd") %>%
rename(date_pulled = date,
return = yearly.returns,
ticker = symbol) %>%
select(ticker, date_pulled, return, type)
View(ytd_return)
# Calculate rolling 6 month return
sixmo_return <- stocks %>%
group_by(symbol) %>%
filter(date >= six_months) %>%
filter(!is.na(close)) %>%
tq_transmute(select = close,
mutate_fun = periodReturn,
period = "yearly",
type = "arithmetic") %>%
mutate(type = "rolling_6_mo") %>%
rename(date_pulled = date,
return = yearly.returns,
ticker = symbol) %>%
select(ticker, date_pulled, return, type)
# compile year to date and 6 month returns, reshape wide
returns <- bind_rows(ytd_return, sixmo_return) %>%
pivot_wider(names_from = type, values_from = return) %>%
ungroup() %>%
mutate(rank_ytd = rank(desc(ytd)),
rank_6mo = rank(desc(rolling_6_mo))) %>%
arrange(rank_ytd)
View(returns)
View(returns)
returns_full <- full_join(funds, returns, by = "ticker") %>%
mutate(ytd_adj       = ytd - expense_ratio * percent_year,
rolling_adj   = rolling_6_mo - expense_ratio / 2,
rank_ytd      = rank(desc(ytd_adj)),
rank_rollling = rank(desc(rolling_adj))) %>%
arrange(rank_ytd)
# adjust ytd and 6 month returns by expense ratio and merge with fund details
percent_year <- as.numeric(today - year_start)/365
returns_full <- full_join(funds, returns, by = "ticker") %>%
mutate(ytd_adj       = ytd - expense_ratio * percent_year,
rolling_adj   = rolling_6_mo - expense_ratio / 2,
rank_ytd      = rank(desc(ytd_adj)),
rank_rollling = rank(desc(rolling_adj))) %>%
arrange(rank_ytd)
View(returns_full)
